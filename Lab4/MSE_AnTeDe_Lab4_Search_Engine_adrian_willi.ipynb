{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Solution of Adrian Willi & Florian Bär\n",
        "- adrian.willi@hslu.ch\n",
        "- florian.baer@hslu.ch\n",
        "\n"
      ],
      "metadata": {
        "id": "DM0aLWTZNlG_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7m2npBBMsT"
      },
      "source": [
        "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
        "\n",
        "# AnTeDe Lab 4: Search Engine with the Vector Space Model\n",
        "\n",
        "## Summary\n",
        "The aim of this lab is to build a simple document search engine based on TF-IDF document vectors. \n",
        "\n",
        "The lab is inspired by a notebook designed by [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb).\n",
        "\n",
        "<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJzlUAgaRs9Y",
        "outputId": "892d5a8a-1777-463a-a921-3b510b01463d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 14.0 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 83.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Modify path according to your configuration\n",
        "# !ls \"/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\"\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks')\n",
        "\n",
        "from TextPreprocessor import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GsWwuZhRAYp",
        "outputId": "54f64622-3867-4d00-d55b-ebac1b751f0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FSpslBouBMsZ"
      },
      "outputs": [],
      "source": [
        "import os    \n",
        "import nltk  # on Colab, you mind find it helpful to run nltk.download('popular') to install packages\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "#from TextPreprocessor import *\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from gensim import models, corpora, similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPjeNW4_BMsb"
      },
      "source": [
        "The data used in this lab is a set of 300 documents selected from the Australian Broadcasting Corporation's news mail service. It consists of texts of headline stories from around the years 2000-2001.  This is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF).  It is available as test data in the **gensim** package, so you do not need to download it separately.\n",
        "\n",
        "The following code will load the documents into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k2Zn3mLxBMsc"
      },
      "outputs": [],
      "source": [
        "# Code inspired from:\n",
        "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
        "\n",
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
        "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
        "text = open(lee_train_file).read().splitlines()\n",
        "data_df = pd.DataFrame({'text': text})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghmCnu2_BMsd"
      },
      "source": [
        "The following code will run our in-house Text Preprocessor provided in the `TextPreprocessor.py` file, and documented in the `MSE_AnTeDe_TextPreprocessingDemo.ipynb` notebook provided in Lab 1 (see Lab 1 archive on Moodle for both files).\n",
        "\n",
        "<font color='green'>Please enrich the following code according your needs (especially stopwords)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai4_wgvuBMsd",
        "outputId": "ba1bc902-72e2-4319-8998-eac26f417dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "language = 'english'\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words(language))\n",
        "# Extend the list here:\n",
        "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
        "    stop_words.add(sw)\n",
        "\n",
        "processor = TextPreprocessor(\n",
        "# Add options here:\n",
        "    language = language,\n",
        "    pos_tags = {wordnet.ADJ, wordnet.NOUN},\n",
        "    stopwords = stop_words\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r89R7_tmBMse"
      },
      "outputs": [],
      "source": [
        "data_df['processed'] = processor.transform(data_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbgZIGX5BMsf",
        "outputId": "600f05bb-1de7-4b82-9afc-f487a76dd75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new report suggests cost australian population exaggerated report australia institute detailed examination population health data show population create unsustainable burden workforce economic social burden found majority old people healthy independent life many financial contribution family voluntary community activity paper challenge assumption old population health cost rise unsustainable level health cost factor growth medical technology consumer demand price\n"
          ]
        }
      ],
      "source": [
        "print(data_df['processed'].iloc[136])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ziwiWhBMsg"
      },
      "source": [
        "## Generation of document vectors with [Scikit-learn](https://scikit-learn.org/stable)\n",
        "\n",
        "We will use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class from scikit-learn to create a vocabulary and generate word counts or *Term Frequencies* (TF).\n",
        "    \n",
        "The result is a  matrix representation of the counts: each column represents a _word_ in the vocabulary and each row represents a document in our dataset: the cell values are the word counts of the word in the document. \n",
        "\n",
        "The matrix is very sparse, because all words not appearing in a document have 0 counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XP55yKgXBMsh"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(max_features=3000) # keep only the 3000 most frequent words in the corpus\n",
        "word_count_vector = cv.fit_transform(data_df['processed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvMcAPg1BMsi"
      },
      "source": [
        "Let's look at some words from our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgHOeNQqBMsi",
        "outputId": "ba2505ca-2012-4e86-91c6-b8c741e0717a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "feature_names = cv.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJA1BPyWBMsj",
        "outputId": "47c1262e-d925-4b42-e4d9-7fa48bf9537e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n",
            "['sri', 'st', 'stabbed', 'stabilise', 'stability']\n",
            "1315\n"
          ]
        }
      ],
      "source": [
        "print(len(feature_names)) # has the max_features value been reached?\n",
        "print(feature_names[2500:2505]) # try various slices\n",
        "print(feature_names.index('hundred')) # find a word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kVKuoQ9BMsj"
      },
      "source": [
        "**TfidfTransformer to Compute Inverse Document Frequency (IDF)**\n",
        "\n",
        "We now use the (sparse) matrix generated by `CountVectorizer` to compute the IDF values of each word.  Note that the IDF should in reality be based on a large and representative corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXDOYgA0BMsk",
        "outputId": "5835786e-e1b2-4c5b-bee7-ff78745f5084"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmeZgkYrBMsk"
      },
      "source": [
        "The IDF values are stored in the `idf_` field of the `TfidfTransformer`.  It has the same size as the array of feature names (words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1sOCgMcBMsk",
        "outputId": "37cab667-9b48-464d-aad3-d24f4b1bbfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n",
            "3.816738506852711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "print(len(tfidf_transformer.idf_)) # check length\n",
        "print(tfidf_transformer.idf_[cv.get_feature_names().index('hundred')]) # check IDF value of a word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vhpBvXfBMsl"
      },
      "source": [
        "**We define here two helper functions:**\n",
        " * the first one is a sorting function for the columns of a sparse matrix in COOrdinate format (a.k.a \"ijv\" or \"triplet\" format [explained here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html));\n",
        " * the second one extracts the feature names (*words*) and their TF-IDF values from the sorted list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qeNxqLa5BMsl"
      },
      "outputs": [],
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items from sorted list\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbOF1UmtBMsl"
      },
      "source": [
        "We now select a document for which we will generate TF-IDF values.  <font color=\"green\">Please select a random document of your choice between 0 and 300.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "liMtNUutBMsm"
      },
      "outputs": [],
      "source": [
        "doc_orig = data_df['text'].iloc[100]\n",
        "doc_processed = data_df['processed'].iloc[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1y-Vs7VBMsm"
      },
      "source": [
        "**The next instruction generates the vector of TF-IDF values for the document** using the `tfidf_transformer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1BwcNOi8BMsm"
      },
      "outputs": [],
      "source": [
        "tf_idf_vector = tfidf_transformer.transform(cv.transform([doc_processed]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isWumU71BMsn"
      },
      "source": [
        "Next, we sort the words in the `tf_idf_vector` by decreasing TF-IDF values, first transforming the vector into a coordinate format ('coo'), and then applying our sorting function from above.  We then extract the words with the top 10 scores (and the scores) for the selected document using our second helper function from above and display them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usGBNnQ8BMsn",
        "outputId": "9990569a-ded6-4ca9-a5ef-0d41c98e6d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Northern Territory's coroner has found that an Aboriginal boy who died in custody nearly two years ago should not have to be in detention. In February last year, the teenager died in hospital from compression of the neck after hanging himself in the Don Dale Juvenile Detention Centre. In his report, the coroner, Dick Wallace, said the 15-year-old was a lonely and negelected orphan who had been offending since he was 13. In January last year, the teenager was given a mandatory 28-day sentence for stealing stationery. However, the coroner said the boy had been eligible to undertake a diversionary program of victim-offender conferencing instead of being sent to detention. The court had not considered that option and neither the prosecutor nor the boy's lawyer had told the magistrate there was an alternative to a custodial sentence.  \n",
            " {'coroner': 0.431, 'detention': 0.346, 'boy': 0.307, 'teenager': 0.261, 'sentence': 0.226, 'year': 0.206, 'stationery': 0.154, 'prosecutor': 0.144, 'neither': 0.144, 'neck': 0.144}\n"
          ]
        }
      ],
      "source": [
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "topn_words = extract_topn_from_vector(feature_names, sorted_items, 10)\n",
        "\n",
        "print(doc_orig, '\\n', topn_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgyagI8WBMsn"
      },
      "source": [
        "<font color=\"green\">Please comment briefly on the relevance of these words with respect to the document content.</font>\n",
        "- - -\n",
        "These top words are the most informative in this document and should summarize the content of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCca4yKMBMsn"
      },
      "source": [
        "## Document-document similarity using scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loByS_VNBMso"
      },
      "source": [
        "In this section, you will write the commands to compute a document-document similarity matrix over the above documents, in scikit-learn.\n",
        "\n",
        "Please use a processing [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) and a [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and compute the *cosine similarities* between all documents.  \n",
        "\n",
        "<font color=\"green\">At the end, you will be asked to display the five most similar documents to the one you selected above, and compare the 1st and the 5th best results.</font>\n",
        "\n",
        "You can use inspiration from: \n",
        " * the above code\n",
        " * https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XkK2ceFCe-Y\n",
        " * https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
        " * https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\n",
        " * https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y6CGf5zOBMso"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VX3SYAfCBMso"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(use_idf=True)\n",
        "pipe = Pipeline(steps=[('pre', processor), ('tfidf', tfidf)]) # the 'processor' was defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HUDN0RBBMso"
      },
      "source": [
        "<font color='green'>Please write a function called `find_similar` which receives a `tfidf_matrix` with all similarity scores between documents, and the `index` of a document in the collection, and returns the `top_n` most similar documents to it using cosine similarity.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dErfJZWBBMso"
      },
      "outputs": [],
      "source": [
        "def find_similar(tfidf_matrix, index, top_n = 5):\n",
        "    sim = cosine_similarity(tfidf_matrix[index], tfidf_matrix).flatten()\n",
        "    return sim.argsort()[:-top_n:-1], sorted(sim)[:-top_n:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4vL2hVUBMsp"
      },
      "source": [
        "<font color=\"green\">Using the data from the Pandas form created above, please use \"fit\" and \"transform\" to generate the matrix of all document similarites called \"tfidf_matrix\". -- How long do these two operations take on your computer?  -- Please explain briefly in your own words what is the difference between \"fit\" and \"transform\".</font>\n",
        "- - -\n",
        "To run both operations 12.27 seconds were used.\n",
        "\n",
        "\n",
        "Fit means that the defined pipeline will be learnt, e.g. parameters are tuned. Transform means that the given data will be passed through the pipeline and transformed using the learnt parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15zRff3mBMsp",
        "outputId": "64689fda-920c-479b-8689-f857d30f8675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 12.270748853683472 seconds ---\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "pipe.fit(data_df['processed'])\n",
        "tfidf_matrix = pipe.transform(data_df['processed'])\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPTAnXXXBMsp"
      },
      "source": [
        "<font color=\"green\">Using `find_similar` and the `tfidf_matrix` please display the five most similar documents to the one you selected above, with their scores, comment them, and compare the 1st and the 5th best results.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FHqBr2iiBMsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37cf486-4d04-4978-dedc-6016b68e3caf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([100,  84, 157,  21, 181]),\n",
              " [1.0,\n",
              "  0.1268368857385407,\n",
              "  0.12004928636145872,\n",
              "  0.10876057591187069,\n",
              "  0.10409143728876288])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "find_similar(tfidf_matrix, 100, top_n=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6shmutLKBMsp"
      },
      "source": [
        "<font color='green'>Could you also use the dot product instead of the cosine similarity in the `find_similar` function?  Please answer in the following box.</font>\n",
        "- - -\n",
        "Yes the dot product can be used to compute the cosine similarity. To acheive this, one first needs to normalize the vectors and then compute the dot product. This corresponds to the cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQTSvePvBMsq"
      },
      "source": [
        "## Building a search engine using Gensim\n",
        "\n",
        "<font color='green'>Using the [tutorial on Topics and Transformations from Gensim](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py), please implement a method that returns the documents most similar to a given query.\n",
        "    \n",
        "Use [Gensim's TF-IDF Model](https://radimrehurek.com/gensim/models/tfidfmodel.html) to build the model and the [MatrixSimilarity function](https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.MatrixSimilarity) to measure cosine similarity between documents.</font>\n",
        "\n",
        "<font color='green'>Please write a query of your own (5-10 words), retrieve the 5 most similar documents, and comment the result.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VFNWp9EpBMsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e54715-e43a-4721-d94c-72bd4786f8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding similar documents for index: 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100    northern territory coroner found aboriginal bo...\n",
              " 84     two asylum seeker woomera detention centre cur...\n",
              " 157    british man found guilty unanimous verdict kid...\n",
              " 21     nation road toll risen another death new south...\n",
              " 181    australian transport safety bureau overnight c...\n",
              " Name: processed, dtype: object,\n",
              " [1.0,\n",
              "  0.1268368857385407,\n",
              "  0.12004928636145872,\n",
              "  0.10876057591187069,\n",
              "  0.10409143728876288])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def most_similar_documents(tfidf_matrix, index, top_n = 6):\n",
        "    print('Finding similar documents for index: %s' % index)\n",
        "    sim_idx, sim_score = find_similar(tfidf_matrix, index, top_n)\n",
        "    docs = data_df['processed'].iloc[sim_idx]\n",
        "    return docs, sim_score\n",
        "\n",
        "most_similar_documents(tfidf_matrix, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZWBQRUABMsq"
      },
      "source": [
        "## End of Lab 4\n",
        "Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cours",
      "language": "python",
      "name": "cours"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MSE_AnTeDe_Lab4_Search_Engine_adrian_willi.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}