{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dfO9WDFWB_M"
   },
   "source": [
    "# AnTeDe Lab D: Your own HMM PoS tagger\n",
    "\n",
    "## Session goal\n",
    "The goal of this session is build your own PoS tagger. This notebook is based on a programming assignment written by Konstantin Taranov \n",
    "and Ondrej Skopek for a Natural Language Understanding course offered at ETH Zurich in spring 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ye2uwLuEWB_N",
    "outputId": "412f9f5c-8446-48c2-a93c-4de532032296"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\adria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\adria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import brown as corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX9wSo9KWB_P"
   },
   "source": [
    "We place START and STOP tags in between the individual sentences in the corpus to avoid mixing things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufKiJpyfWB_P",
    "outputId": "5195d893-119e-4df9-e74b-0623b51204a2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DET': 0.21342867108475758, 'NOUN': 0.1411405650505755, 'ADJ': 0.034339030345308684, 'VERB': 0.04513428671084758, 'ADP': 0.1228461806766655, '.': 0.08892570631321939, 'ADV': 0.0913498430415068, 'CONJ': 0.049128008371119636, 'PRT': 0.036675967910708054, 'PRON': 0.15969654691314963, 'NUM': 0.016811998604813395, 'X': 0.0005231949773282176}\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "tagged_words = []\n",
    "all_tags = []\n",
    "\n",
    "starting_p={}\n",
    "count=0\n",
    "\n",
    "sentences =corpus.tagged_sents(tagset='universal')\n",
    "\n",
    "for sent in sentences:\n",
    "    tagged_words.append((\"START\", \"START\"))\n",
    "    all_tags.append(\"START\")\n",
    "    start=True\n",
    "    for word, tag in sent:\n",
    "        try:\n",
    "            starting_p[tag]\n",
    "        except:\n",
    "            starting_p[tag]=0\n",
    "        if start:\n",
    "            starting_p[tag]=starting_p[tag]+1\n",
    "            count=count+1\n",
    "            start=False\n",
    "            \n",
    "        all_tags.append(tag)\n",
    "        tagged_words.append((tag, word))\n",
    "    tagged_words.append((\"END\", \"END\"))\n",
    "    all_tags.append(\"END\")\n",
    "\n",
    "for tag in starting_p.keys():\n",
    "    starting_p[tag]=starting_p[tag]/count\n",
    "    \n",
    "print (starting_p)\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5A4txTeWB_Q",
    "outputId": "60bbc114-52ef-404e-b3aa-a0dcf3e0a394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('START', 'START'), ('DET', 'The'), ('NOUN', 'Fulton'), ('NOUN', 'County'), ('ADJ', 'Grand'), ('NOUN', 'Jury'), ('VERB', 'said'), ('NOUN', 'Friday'), ('DET', 'an'), ('NOUN', 'investigation')]\n",
      "['START', 'DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print (tagged_words[0:10])  \n",
    "print (all_tags[0:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJbUAUGJWB_R"
   },
   "source": [
    "We use NLTK to estimate the transition probabilities and the emission probabilities as conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D31r9MnwWB_R"
   },
   "source": [
    "Compute $P(t_{i} | t_{i-1})= \\frac{C(t_{i-1}, t_{i})}{C(t_{i-1})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tP1Ch3K8WB_R"
   },
   "outputs": [],
   "source": [
    "cfd_tags = nltk.ConditionalFreqDist(nltk.bigrams(all_tags))\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkN-JnMNWB_S",
    "outputId": "ac56bf18-f99c-408c-ec65-4b7467e0e192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADV', 'ADJ') appears 7666  times\n",
      "ADV appears 56239 times\n",
      "The probability of P('ADJ'|'ADV') is roughly 0.1363\n"
     ]
    }
   ],
   "source": [
    "print(\"('ADV', 'ADJ') appears\",\n",
    "      cfd_tags['ADV']['ADJ'], \" times\" )\n",
    "\n",
    "print (\"ADV appears \"+str(cfd_tags['ADV'].N())+\" times\")\n",
    "\n",
    "\n",
    "# P('ADJ' | 'ADV')\n",
    "print(\"The probability of P('ADJ'|'ADV') is roughly\",\n",
    "      round(cpd_tags['ADV'].prob('ADJ'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKRHuuP0WB_T"
   },
   "source": [
    "Compute $P(w_{i} | t_{i}) =  \\frac{C(t_{i}, w_{i})}{C(t_{i})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9ao-s1OGWB_T"
   },
   "outputs": [],
   "source": [
    "cfd_tw = nltk.ConditionalFreqDist(tagged_words)\n",
    "cpd_tw = nltk.ConditionalProbDist(cfd_tw, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZO6Vsug-WB_U",
    "outputId": "d5b24c1d-686e-4939-db22-71d38e5562d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of C('NOUN', 'dog') is 70\n",
      "Probability of P('dog' | 'NOUN') is 0.0002540300045725401\n"
     ]
    }
   ],
   "source": [
    "# C('dog', 'NOUN'):\n",
    "print(\"Frequency of C('NOUN', 'dog') is\",\n",
    "      cfd_tw['NOUN']['dog'])\n",
    "\n",
    "# P('dog' | 'NOUN')\n",
    "print(\"Probability of P('dog' | 'NOUN') is\",\n",
    "      cpd_tw['NOUN'].prob('dog') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88RgEwmWB_U"
   },
   "source": [
    "Here follows the code for the Viterbi algorithm, adapted from 8c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "F16DRk9oWB_V"
   },
   "outputs": [],
   "source": [
    "def viterbi(observations, states, starting_p, transition_p, emission_p):\n",
    "    \n",
    "    # your trellis is a list of dictionaries\n",
    "    trellis = [{}]\n",
    "\n",
    "    # first column of the trellis: \n",
    "    # how likely you are to start in each state, multiplied by \n",
    "    # how likely you are to generate the initial observation \n",
    "    # from each state\n",
    "    \n",
    "    \n",
    "\n",
    "    for state in states:\n",
    "        trellis[0][state] = \\\n",
    "        {\"probability\":\\\n",
    "         starting_p[state] * emission_p[state][observations[0]],\\\n",
    "               \"previous state\": None}\n",
    "\n",
    "        \n",
    "    # for loop over the trellis columns, left to right\n",
    "    for k in range(1, len(observations)):\n",
    "\n",
    "        # add a column\n",
    "        new_column = {}\n",
    "\n",
    "        # for each row in the column\n",
    "        for state in states:\n",
    "            \n",
    "            max_path_p=0\n",
    "            \n",
    "\n",
    "            for previous_state in states:\n",
    "\n",
    "                up_to_here_p =\\\n",
    "                    trellis[k-1][previous_state][\"probability\"]*\\\n",
    "                    transition_p[previous_state][state]\n",
    "\n",
    "                if up_to_here_p > max_path_p:\n",
    "\n",
    "                    max_path_p = up_to_here_p\n",
    "\n",
    "                    prev_st_selected = previous_state\n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "            max_p = max_path_p * emission_p[state][observations[k]]\n",
    "\n",
    "            \n",
    "            \n",
    "            new_column[state]=\\\n",
    "            {\"probability\": max_p,\\\n",
    "             \"previous\": prev_st_selected}\n",
    "            \n",
    "        trellis.append(new_column)\n",
    "\n",
    "    max_prob = max(value[\"probability\"]\\\n",
    "                   for value in trellis[-1].values())\n",
    "\n",
    "    previous = None\n",
    "\n",
    "    return trellis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p3Fr8h8WB_V"
   },
   "source": [
    "Here we get our transmission probabilities in the same format we had them in notebook 8c. Let's check we still have the same P('ADJ'|'ADV') as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aPMganeWB_W",
    "outputId": "d5b497e5-bf13-4c78-b6aa-d32c50689114",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']\n",
      "0.13631110083749712\n"
     ]
    }
   ],
   "source": [
    "tagset = list(starting_p.keys())\n",
    "print (tagset)\n",
    "\n",
    "transition_p={}\n",
    "for tag in tagset:\n",
    "    transition_p[tag]={}\n",
    "    for conditioned_on_tag in tagset:\n",
    "        transition_p[tag][conditioned_on_tag]=\\\n",
    "        cpd_tags[conditioned_on_tag].prob(tag)\n",
    "\n",
    "\n",
    "print (transition_p['ADJ']['ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3ukww7LWB_W"
   },
   "source": [
    "Here you can use your code from 8c to get the PoS tag sequence chosen by your tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_p8fWAOMWB_X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tag_sequence (trellis, observations):\n",
    "# BEGIN_REMOVE\n",
    "    import numpy as np\n",
    "    probs=[trellis[-1][state]['probability'] for state in trellis[-1].keys()]\n",
    "    chosen=np.max(np.asarray(probs))\n",
    "    chosen_state=[(state, trellis[-1][state]['previous'])\\\n",
    "                  for state in trellis[-1].keys() if trellis[-1][state]['probability']==chosen]\n",
    "\n",
    "    opt=[chosen_state[0][0]]\n",
    "    previous=chosen_state[0][1]\n",
    "\n",
    "    for t in range(len(trellis) - 2, -1, -1):\n",
    "        opt.insert(0, trellis[t + 1][previous][\"previous\"])\n",
    "        previous = trellis[t + 1][previous][\"previous\"]\n",
    "\n",
    "    result=[]    \n",
    "    for i, word in enumerate(observations):\n",
    "        result.append((word, opt[i]))\n",
    "    return result    \n",
    "        \n",
    "# END_REMOVE        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB_SBAyDWB_X"
   },
   "source": [
    "Here's the function that we will call to run our PoS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uEUHua_pWB_X"
   },
   "outputs": [],
   "source": [
    "import nltk.tokenize as tokenize\n",
    "\n",
    "def hmm_postagger (sentence):\n",
    "    observations = tokenize.word_tokenize(sentence)\n",
    "    \n",
    "\n",
    "    emission_p={}\n",
    "    for tag in tagset:\n",
    "        emission_p[tag]={}\n",
    "        for word in observations:\n",
    "            emission_p[tag][word]=cpd_tw[tag].prob(word)\n",
    "\n",
    "    my_trellis = viterbi (observations, tagset, starting_p, transition_p, emission_p)\n",
    "    return get_pos_tag_sequence(my_trellis, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfjLxqaFWB_Y"
   },
   "source": [
    "Let's test our PoS tagger. Come up with other test sentences to see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64U3jLMhWB_Y",
    "outputId": "d698414b-85cd-468a-ed9c-7cec887079a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'DET'), ('dog', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('dog', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"My dog is a really good dog.\"\n",
    "result = hmm_postagger (sentence)\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MSE_AnTeDe_Lab_d_yourHMM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
