{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Solution of Adrian Willi & Florian Bär\n",
        "- adrian.willi@hslu.ch\n",
        "- florian.baer@hslu.ch\n",
        "\n"
      ],
      "metadata": {
        "id": "X7zzZVaz0QxJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvh_uwAoVhcI"
      },
      "source": [
        "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
        "\n",
        "# AnTeDe Lab 5: Latent Semantic Analysis with Gensim\n",
        "\n",
        "## Objective\n",
        "The goal of this lab is to perform LSA on a small corpus of news.  You will use the LSA word vectors to estimate word similarity, and then to perform ranked retrieval given a query. \n",
        "\n",
        "<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXxbyRHjVkAk",
        "outputId": "641646da-9acb-47b9-e2df-913516301def"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 31.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Modify path according to your configuration\n",
        "# !ls \"/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\"\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks')\n",
        "\n",
        "from TextPreprocessor import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnWjBjghVn6m",
        "outputId": "2bca3698-76e9-4720-8f88-c698ea36c312"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZEsqn17FVhcT"
      },
      "outputs": [],
      "source": [
        "import os    \n",
        "import nltk\n",
        "import gensim\n",
        "import pandas as pd\n",
        "#from TextPreprocessor import *\n",
        "from gensim import models, corpora, similarities\n",
        "from gensim.models import LsiModel, LdaModel, LdaMulticore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ2yVQOJVhcX"
      },
      "source": [
        "The data used in this lab the same set of 300 Australian that you used in Lab 4 on Document Representation.  It is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF) and it is available with the **gensim** package that you installed.  The following code will load the documents into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2wdgEWH3VhcY"
      },
      "outputs": [],
      "source": [
        "# Code inspired from:\n",
        "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
        "\n",
        "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
        "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
        "text = open(lee_train_file).read().splitlines()\n",
        "data_df = pd.DataFrame({'text': text})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdTbzN6oVhca"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "You will need first to preprocess the data through the following stages:\n",
        "1. tokenization\n",
        "2. stopword removal\n",
        "2. POS-based filtering (optional)\n",
        "3. lemmatization or stemming (optional)\n",
        "4. addition of bigrams to each document (optional)\n",
        "5. filtering of infrequent words\n",
        "6. inspection and filtering of frequent words\n",
        "\n",
        "You can use NLTK or our in-house `TextPreprocessor.py` file, as explained in Lab 1.\n",
        "\n",
        "<font color='green'>Please state here which solution you use and list stages you implement.</font>\n",
        "- - -\n",
        "1. tokenization\n",
        "2. stopword removal\n",
        "3. lemmatization\n",
        "4. filtering of infrequent words\n",
        "5. inspection and filtering of frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RycUyDqCVhce",
        "outputId": "daee5721-08b7-4a6b-c63a-4c5afd4b2ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "# Please write here the preprocessing instructions if you use TextPreprocessor.py\n",
        "language = 'english'\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words(language))\n",
        "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
        "    stop_words.add(sw)\n",
        "\n",
        "processor = TextPreprocessor(\n",
        "# Add options here:\n",
        "    language = language,\n",
        "    stopwords = stop_words,\n",
        "    lemmatize = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7VbhefcFVhch"
      },
      "outputs": [],
      "source": [
        "data_df['processed'] = processor.transform(data_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['processed'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkwemkEiagJP",
        "outputId": "1ca997a2-4e98-4903-8bd2-732ca2d911e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    hundred people force vacate home southern high...\n",
              "1    indian security force shot dead eight suspect ...\n",
              "2    national road toll christmas-new year holiday ...\n",
              "3    argentina political economic crisis deepen res...\n",
              "4    six midwife suspend wollongong hospital south ...\n",
              "Name: processed, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n7KWcNJgVhck"
      },
      "outputs": [],
      "source": [
        "data_df['tokenized'] = data_df['processed'].apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['tokenized'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKns-7imazDr",
        "outputId": "33ab4e91-3e22-40d4-d9a3-9c21fdd4d1f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [hundred, people, force, vacate, home, souther...\n",
              "1    [indian, security, force, shot, dead, eight, s...\n",
              "2    [national, road, toll, christmas-new, year, ho...\n",
              "3    [argentina, political, economic, crisis, deepe...\n",
              "4    [six, midwife, suspend, wollongong, hospital, ...\n",
              "Name: tokenized, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O4nxBsPCVhco"
      },
      "outputs": [],
      "source": [
        "# Alternatively, please write here the preprocessing instructions if you use NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7YSJrAYVhcp",
        "outputId": "ca9c597b-7a01-4479-e0d1-61bc50bc84c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['union', 'represent', 'qantas', 'maintenance', 'worker', 'warn', 'escalate', 'industrial', 'action', 'company', 'reject', 'offer', 'long', 'run', 'dispute', 'arbitrate', 'party', 'lock', 'private', 'talk', 'yesterday', 'industrial', 'relation', 'commission', '3,000', 'maintenance', 'worker', 'earlier', 'vote', 'reject', 'qantas', 'propose', 'wage', 'freeze', 'national', 'secretary', 'australian', 'manufacturing', 'worker', 'union', 'amwu', 'doug', 'cameron', 'say', 'union', 'do', 'everything', 'possible', 'resolve', 'dispute', 'qantas', 'prepared', 'accept', 'private', 'arbitration', 'absolutely', 'alternative', 'worker', 'take', 'industrial', 'action', 'escalate', 'industrial', 'action', 'necessary', 'ensure', 'get', 'fair', 'go', 'company', 'seem', 'determine', 'crush', 'underfoot', 'say']\n"
          ]
        }
      ],
      "source": [
        "print(data_df['tokenized'].iloc[120])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR8KTVg9Vhcr"
      },
      "source": [
        "Please make a list of all words from all articles.  Then, using `nltk.FreqDist`, consider the most frequent and the least frequent words.  If you find uninformative words among the most frequent ones, please remove them from the articles.  Similarly, please remove from articles the words appearing fewer than 2 or 3 times in the corpus.  <font color='green'> Please justify these choices. What is now the size of your vocabulary?</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th1FqAjmVhcu",
        "outputId": "4ada2745-5bb3-4526-9d2a-8d5c8a4c9a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent words (n=50): \n",
            " [('say', 1011), ('mr', 306), ('australian', 178), ('new', 171), ('palestinian', 168), ('australia', 157), ('people', 153), ('government', 146), ('attack', 140), ('two', 136), ('u', 136), ('day', 131), ('south', 130), ('state', 129), ('force', 126), ('year', 126), ('would', 115), ('take', 114), ('one', 114), ('israeli', 112), ('also', 111), ('minister', 106), ('fire', 103), ('last', 102), ('first', 102), ('arafat', 96), ('go', 94), ('make', 92), ('afghanistan', 90), ('united', 89), ('three', 87), ('police', 86), ('world', 84), ('security', 83), ('time', 83), ('official', 83), ('could', 82), ('report', 81), ('call', 80), ('kill', 80), ('area', 79), ('give', 78), ('today', 77), ('leader', 77), ('group', 75), ('told', 75), ('come', 74), ('get', 74), ('company', 73), ('union', 71)]\n",
            "\n",
            "Least frequent words (n=50): \n",
            " [('uk', 1), ('mysticism', 1), ('dawn', 1), ('tent', 1), ('marquee', 1), ('rob', 1), ('terminate', 1), ('throw', 1), ('basically', 1), ('unwind', 1), ('recrimination', 1), ('congressman', 1), ('excess', 1), ('precursor', 1), ('inject', 1), ('mouse', 1), ('tissue', 1), ('variety', 1), ('parkinson', 1), ('epidemic', 1), ('annually', 1), ('ukraine', 1), ('insecurity', 1), ('steep', 1), ('merger', 1), ('stone', 1), ('muscle', 1), ('grouping', 1), ('merge', 1), ('categorically', 1), ('academic', 1), ('centenary', 1), ('recognises', 1), ('lecturer', 1), ('bedeharris', 1), ('monarchy', 1), ('codification', 1), ('opporunity', 1), ('consult', 1), ('cedric', 1), ('pioline', 1), ('fabrice', 1), ('santoro', 1), ('appraisal', 1), ('pro', 1), ('con', 1), ('salvage', 1), ('overcame', 1), ('sebastien', 1), ('grosjean', 1)]\n",
            "\n",
            "New least frequent words (n=10): \n",
            " [('hornsby', 4), ('ganges', 4), ('hindu', 4), ('beatles', 4), ('saxeten', 4), ('liverpool', 4), ('electricity', 4), ('arthur', 4), ('rubber', 4), ('cow', 4)]\n",
            "\n",
            "Before filtering: 5336\n",
            "After filtering: 1808\n"
          ]
        }
      ],
      "source": [
        "# Please write here all the necessary instructions.  You may use several cells.\n",
        "\n",
        "# remove non-alphanumeric words\n",
        "word_list = [w for ws in data_df['tokenized'] for w in ws if w.isalpha()]\n",
        "\n",
        "fdist = nltk.FreqDist(word_list)\n",
        "\n",
        "# most frequent\n",
        "print('Most frequent words (n=50): \\n', fdist.most_common(50)) \n",
        "\n",
        "# least frequent\n",
        "print('\\nLeast frequent words (n=50): \\n', fdist.most_common()[-50:]) \n",
        "\n",
        "# remove words that have an occurence of < 3\n",
        "filtered = dict((word, freq) for word, freq in fdist.items() if freq > 3)\n",
        "fdist_filtered = nltk.FreqDist(filtered)\n",
        "print('\\nNew least frequent words (n=10): \\n', fdist_filtered.most_common()[-10:]) \n",
        "\n",
        "# check vocab size\n",
        "vocab = set(word for word, _ in fdist.items())\n",
        "vocab_filtered = set(word for word, _ in fdist_filtered.items())\n",
        "print('\\nBefore filtering: %s' % len(vocab))\n",
        "print('After filtering: %s' % len(vocab_filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKOwmlxsVhcw",
        "outputId": "07a44480-18da-4925-c3b0-35dc7b2697a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['afghan', 'security', 'force', 'arrest', 'wound', 'arab', 'al', 'qaeda', 'fighter', 'seven', 'others', 'weapon', 'explosive', 'remain', 'hospital', 'southern', 'city', 'kandahar', 'spokesman', 'governor', 'gul', 'agha', 'say', 'man', 'arrest', 'left', 'ward', 'one', 'arab', 'believe', 'take', 'custody', 'come', 'ward', 'mr', 'say', 'say', 'seven', 'carry', 'weapon', 'include', 'grenade', 'explosive', 'try', 'explosive', 'surrender', 'weapon', 'mr', 'say', 'concerned', 'safety', 'arab', 'wound', 'earlier', 'u', 'bombing', 'kandahar', 'airport', 'admit', 'hospital', 'taliban', 'militia', 'earlier', 'month', 'flee', 'taliban', 'hand', 'weapon', 'include', 'grenade', 'explosive', 'arab', 'could', 'protect', 'threaten', 'blow', 'hospital', 'room', 'attempt', 'make', 'arrest']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [hundred, people, force, home, southern, highl...\n",
              "1      [indian, security, force, shot, dead, eight, s...\n",
              "2      [national, road, toll, year, holiday, period, ...\n",
              "3      [argentina, political, economic, crisis, inter...\n",
              "4      [six, midwife, suspend, hospital, south, sydne...\n",
              "                             ...                        \n",
              "295    [team, australian, israeli, scientist, conduct...\n",
              "296    [today, world, aid, day, late, figure, show, m...\n",
              "297    [federal, national, party, reject, possible, s...\n",
              "298    [university, canberra, proposal, republic, one...\n",
              "299    [australia, take, france, double, rubber, davi...\n",
              "Name: filtered, Length: 300, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# filter the data\n",
        "data_df['filtered'] = [[w for w in ws if w in vocab_filtered] for ws in data_df['tokenized']]\n",
        "print(data_df['filtered'].iloc[50])\n",
        "data_df['filtered']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-AGtbhcVhcx"
      },
      "source": [
        "## LSA with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr9Fw-KHVhcy"
      },
      "source": [
        "In this section, you will write the Gensim commands to compute a term-document matrix from the above documents, then transform it using SVD, and truncate the result.  To learn what the commands are, please follow the [Topics and Tranformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) from Gensim. \n",
        "\n",
        "<font color=\"green\">Please gather these commands into a function called `train_lsa`.  They should cover: dictionary creation, corpus mapping, computation of TF-IDF values, and creation of the LSA model.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xREwntv-Vhc0"
      },
      "outputs": [],
      "source": [
        "def train_lsa(filtered_texts, num_topics = 10):\n",
        "\n",
        "  # create dictionary\n",
        "  dictionary = corpora.Dictionary(filtered_texts)\n",
        "\n",
        "  # corpus mapping\n",
        "  corpus = [dictionary.doc2bow(text) for text in filtered_texts]\n",
        "\n",
        "  # TF-IDF model\n",
        "  tfidf = models.TfidfModel(corpus, normalize=True)\n",
        "  corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "  # LSA model\n",
        "  lsa = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "  return lsa, dictionary, corpus, corpus_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qWVzflDVhc1"
      },
      "source": [
        "<font color=\"green\">Please fix a `number_of_topics`, on the lower side of the range mentioned in the course.  Then, execute the cell that performs `train_lsa`.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hmTRPpRjVhc2"
      },
      "outputs": [],
      "source": [
        "number_of_topics = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OJ4R-9K3Vhc2"
      },
      "outputs": [],
      "source": [
        "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], number_of_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzTX6Fs0Vhc5"
      },
      "source": [
        "<font color=\"green\">Please display several topics found by LSA using the Gensim `print_topics` function.  Please explain in your own words the meaning of what is displayed.  How do you relate it with what was explained in the course on LSA?</font>\n",
        "- - -\n",
        "The topics show the latent dimensions of the LSA transformation. The words 'palestinian', 'israeli' and 'arafat' are related and contribute the most to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWvY5vsYVhc8",
        "outputId": "5361f36b-b6ba-45c8-daae-3923255e1d0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.296*\"palestinian\" + 0.196*\"israeli\" + 0.173*\"arafat\" + 0.119*\"mr\" + 0.110*\"attack\" + 0.110*\"israel\" + 0.104*\"hamas\" + 0.099*\"afghanistan\" + 0.096*\"force\" + 0.091*\"u\"'),\n",
              " (1,\n",
              "  '-0.444*\"palestinian\" + -0.295*\"israeli\" + -0.260*\"arafat\" + -0.162*\"israel\" + -0.156*\"hamas\" + -0.139*\"gaza\" + -0.112*\"sharon\" + 0.102*\"afghanistan\" + -0.101*\"suicide\" + -0.093*\"militant\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "lsa_model.print_topics(number_of_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayy4Jl-EVhc_"
      },
      "source": [
        "<font color=\"green\">Please define a function that returns the cosine similarity between two words (testing first if they are in the vocabulary). Please exemplify its value on two different word pairs, one of which should be obviously more similar than the other, and comment the values.</font>  You can get inspiration from this [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html).\n",
        "- - -\n",
        "We see that the similarity of the first one is significantly higher than of the second one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bahImB7mVhdC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SHJuFONNVhdE"
      },
      "outputs": [],
      "source": [
        "def wordsim(word1, word2, model, dictionary):\n",
        "  word1_bow = dictionary.doc2bow([word1])\n",
        "  word2_bow = dictionary.doc2bow([word2])\n",
        "\n",
        "  if len(word1_bow) == 0 or len(word2_bow) == 0:\n",
        "        raise Exception('Words not in dictionary!')\n",
        "\n",
        "  # convert to LSA space\n",
        "  word1_lsa = model[word1_bow]\n",
        "  word2_lsa = model[word2_bow]\n",
        "    \n",
        "  # compute the similarity\n",
        "  sim = cosine_similarity(word1_lsa, word2_lsa)\n",
        "\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EmL62jUVhdF",
        "outputId": "c024c8a8-bf5e-4e46-df05-592b329158cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.2829564 ]\n",
            " [0.25114428 0.99945513]]\n",
            "[[ 1.          0.13786654]\n",
            " [-0.0014011   0.99025668]]\n"
          ]
        }
      ],
      "source": [
        "# print here the cosine similiarities of several pairs and comment the results.\n",
        "sim_high = wordsim('arafat', 'israeli', lsa_model, dictionary)\n",
        "sim_low = wordsim('hindu', 'gaza', lsa_model, dictionary)\n",
        "\n",
        "print(sim_high)\n",
        "print(sim_low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORFYVDLCVhdG"
      },
      "source": [
        "<font color=\"green\">Please use the [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html) to write a function that prints a list of words sorted by decreasing LSA similarity with a given word and showing the score too.  You don't have to use the cosine_similarity function here.  Please choose a \"query\" word and ten other words, apply your function, and comment the results.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDtNynv6VhdH"
      },
      "outputs": [],
      "source": [
        "from gensim import similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PasevLFuVhdH"
      },
      "outputs": [],
      "source": [
        "def word_ranking(word0, word_list, model, dictionary):\n",
        "  word0_bow = dictionary.doc2bow([word0])\n",
        "  words_bow = [dictionary.doc2bow([w]) for w in word_list]\n",
        "    \n",
        "  word0_lsa = model[word0_bow]\n",
        "  words_lsa = model[words_bow]\n",
        "    \n",
        "  index = similarities.MatrixSimilarity(words_lsa)\n",
        "    \n",
        "  # perform a similarity query against the corpus\n",
        "  sims = index[word0_lsa]  \n",
        "  sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "\n",
        "  for i, (doc_position, doc_score) in enumerate(sims):\n",
        "    print('{0}: \"{1}\"\", score: {2:.5f}'.format(i, word_list[doc_position], doc_score))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S1rhf6WVhdI",
        "outputId": "bd1156ca-68fc-4056-ccdf-67ff414c032c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \"hamas\"\", score: 0.99990\n",
            "1: \"palestinian\"\", score: 0.99989\n",
            "2: \"arafat\"\", score: 0.99987\n",
            "3: \"israel\"\", score: 0.99975\n",
            "4: \"hot\"\", score: 0.20795\n",
            "5: \"force\"\", score: 0.14559\n",
            "6: \"hindu\"\", score: -0.04589\n",
            "7: \"black\"\", score: -0.16789\n",
            "8: \"wind\"\", score: -0.29011\n",
            "9: \"water\"\", score: -0.30594\n"
          ]
        }
      ],
      "source": [
        "# call here the function on your choice of words\n",
        "word_ranking('gaza', ['hot', 'water', 'palestinian', 'israel', 'hindu', 'black', 'wind', 'arafat', 'hamas', 'force'], lsa_model, dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SmbrtVwHVhdI"
      },
      "outputs": [],
      "source": [
        "# Please write here your comments on the rankings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be clearly seen that words like 'hamas' and 'arafat' have a high score with 'gaza' because they often appear together. On the other end, words like 'water' and 'wind' have a low score because they don't show up often together.\n"
      ],
      "metadata": {
        "id": "OElxC0ZWKbmb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi4lPfEVVhdJ"
      },
      "source": [
        "<font color=\"green\">Please select now a significantly larger number of topics, and train a new LSA model.  Perform the same `word_ranking` task as above and compare the new ranking with the previous one.  Which one seems better?</font>\n",
        "- - -\n",
        "The small one with 2 topics is much better than the bigger one with 300 topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], num_topics=300)\n",
        "word_ranking('gaza', ['hot', 'water', 'palestinian', 'israel', 'hindu', 'black', 'wind', 'arafat', 'hamas', 'force'], lsa_model, dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDMRREpqK2ni",
        "outputId": "23c7964b-ae11-4fb9-86af-ee35b4b11cc9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \"palestinian\"\", score: 0.14060\n",
            "1: \"arafat\"\", score: 0.13423\n",
            "2: \"force\"\", score: 0.04083\n",
            "3: \"hamas\"\", score: 0.02699\n",
            "4: \"wind\"\", score: 0.00703\n",
            "5: \"hindu\"\", score: 0.00558\n",
            "6: \"water\"\", score: -0.01309\n",
            "7: \"black\"\", score: -0.06254\n",
            "8: \"hot\"\", score: -0.06660\n",
            "9: \"israel\"\", score: -0.09030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwPxGebYVhdJ"
      },
      "source": [
        "## End of Lab 5\n",
        "Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cours",
      "language": "python",
      "name": "cours"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MSE_AnTeDe_Lab5_LSA_Gensim_adrian_willi.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}